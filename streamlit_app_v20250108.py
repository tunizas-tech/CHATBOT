import os
import hashlib
from pathlib import Path
from typing import Optional, List

import streamlit as st

from langchain.text_splitter import RecursiveCharacterTextSplitter
# ì»¤ë®¤ë‹ˆí‹° ê²½ë¡œì˜ FAISS ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# OpenAI LLM/Embedding
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

# Agent ê´€ë ¨
from langchain.tools.retriever import create_retriever_tool
from langchain.prompts import ChatPromptTemplate
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.agents import Tool

# ê²€ìƒ‰(ì˜µì…˜)
from langchain_community.utilities import SerpAPIWrapper

# ========= ê³ ì • ê²½ë¡œ/ì˜µì…˜ =========
PDF_DIR = "./pdf"                 # <- ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤
FAISS_DIR = "./faiss_index"       # ì¸ë±ìŠ¤ ì €ì¥ í´ë”
EMBED_MODEL = "text-embedding-3-small"  # ê²½ëŸ‰ ì„ë² ë”© ëª¨ë¸ ê¶Œì¥
CHUNK_SIZE = 800
CHUNK_OVERLAP = 150

# í™˜ê²½ ì„¤ì •
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


# ========== SerpAPI ì›¹ê²€ìƒ‰ íˆ´(í‚¤ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ ì•ˆ í•¨) ==========
def search_web_tool_or_none():
    api_key = os.environ.get("SERPAPI_API_KEY") or st.session_state.get("SERPAPI_API")
    if not api_key:
        return None

    search = SerpAPIWrapper()  # ë‚´ë¶€ì—ì„œ í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì½ìŒ

    def run_with_source(query: str) -> str:
        try:
            results = search.results(query)
            organic = results.get("organic_results", [])
            formatted = []
            for r in organic[:5]:
                title = r.get("title")
                link = r.get("link")
                source = r.get("source")
                snippet = r.get("snippet")
                if link:
                    formatted.append(f"- [{title}]({link}) ({source})\n  {snippet}")
                else:
                    formatted.append(f"- {title} (ì¶œì²˜: {source})\n  {snippet}")
            return "\n".join(formatted) if formatted else "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    return Tool(
        name="web_search",
        func=run_with_source,
        description="ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë° ì›¹ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ì œëª©+ì¶œì²˜+ë§í¬+ê°„ë‹¨ìš”ì•½(snippet) í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤."
    )


# ========== PDF ë¡œë”: OCR Fallback í¬í•¨(ì„ íƒ) ==========
def load_with_ocr_fallback(pdf_path: Path) -> List[Document]:
    """PyPDFLoader ì‹¤íŒ¨ ì‹œ Unstructured OCRë¡œ ëŒ€ì²´ ì‹œë„"""
    try:
        from langchain.document_loaders import PyPDFLoader
        return PyPDFLoader(str(pdf_path)).load()
    except Exception:
        # OCR fallback (ì„ íƒ): í•„ìš” ì‹œ ì˜ì¡´ì„± ì„¤ì¹˜
        try:
            # pip install "unstructured[local-inference]"
            from langchain_community.document_loaders import UnstructuredPDFLoader
            loader = UnstructuredPDFLoader(str(pdf_path), mode="single")
            return loader.load()
        except Exception as e:
            st.error(f"OCR fallback ì‹¤íŒ¨: {pdf_path.name} - {e}")
            return []


# ========== í´ë” ë³€ê²½ ê°ì§€(íŒŒì¼ ê²½ë¡œ+ìˆ˜ì •ì‹œê°ìœ¼ë¡œ í•´ì‹œ) ==========
def _folder_signature(pdf_dir: Path) -> str:
    files = sorted(pdf_dir.glob("**/*.pdf"))
    h = hashlib.md5()
    for f in files:
        try:
            stat = f.stat()
            h.update(str(f.resolve()).encode())
            h.update(str(int(stat.st_mtime)).encode())
        except Exception:
            # ì ‘ê·¼ ë¶ˆê°€/ì¼ì‹œ ì˜¤ë¥˜ëŠ” ìŠ¤í‚µ
            continue
    return h.hexdigest()


# ========== ìºì‹œ: ì¸ë±ìŠ¤ ë¹Œë“œ ë˜ëŠ” ë¡œë”© ==========
@st.cache_resource(show_spinner=True)
def build_or_load_faiss_index(pdf_dir_str: str) -> Optional[FAISS]:
    pdf_dir = Path(pdf_dir_str)
    if not pdf_dir.exists() or not pdf_dir.is_dir():
        st.warning(f"ì§€ì •í•œ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_dir.resolve()}")
        return None

    sig = _folder_signature(pdf_dir)
    faiss_sig_file = Path(FAISS_DIR) / "signature.txt"

    # ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ìˆê³ , ì‹œê·¸ë‹ˆì²˜ê°€ ë™ì¼í•˜ë©´ ë¡œë“œ
    if Path(FAISS_DIR).exists() and faiss_sig_file.exists():
        old_sig = faiss_sig_file.read_text(encoding="utf-8").strip()
        if old_sig == sig:
            try:
                vs = FAISS.load_local(
                    FAISS_DIR,
                    OpenAIEmbeddings(model=EMBED_MODEL),
                    allow_dangerous_deserialization=True
                )
                return vs
            except Exception:
                # ì¸ë±ìŠ¤ê°€ ê¹¨ì¡Œê±°ë‚˜ ë²„ì „ ì¶©ëŒ ë“±: ì¬ë¹Œë“œë¡œ í´ë°±
                pass

    # ì¬ë¹Œë“œ
    files = list(pdf_dir.glob("**/*.pdf"))
    if not files:
        st.warning(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_dir.resolve()}")
        return None

    all_docs: List[Document] = []
    for pdf in files:
        docs = load_with_ocr_fallback(pdf)
        if not docs:
            st.error(f"PDF ë¡œë”© ì‹¤íŒ¨: {pdf.name}")
            continue
        # ì¶œì²˜ë¥¼ metadataì— ê¸°ë¡
        for d in docs:
            d.metadata = d.metadata or {}
            d.metadata["source"] = pdf.name
            d.metadata["path"] = str(pdf.resolve())
        all_docs.extend(docs)

    if not all_docs:
        st.warning("ë¬¸ì„œ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None

    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(all_docs)

    vs = FAISS.from_documents(chunks, OpenAIEmbeddings(model=EMBED_MODEL))

    Path(FAISS_DIR).mkdir(parents=True, exist_ok=True)
    vs.save_local(FAISS_DIR)
    faiss_sig_file.write_text(sig, encoding="utf-8")
    return vs


# ========== retriever tool ìƒì„± ==========
def make_pdf_search_tool():
    vs = build_or_load_faiss_index(PDF_DIR)
    if not vs:
        return None
    retriever = vs.as_retriever(search_kwargs={"k": 5})
    return create_retriever_tool(
        retriever,
        name="pdf_search",
        description="Use this tool to search information from the pdf document"
    )


# ========== Agent ëŒ€í™” ì‹¤í–‰ ==========
def chat_with_agent(user_input, agent_executor):
    result = agent_executor({"input": user_input})
    return result["output"]


# ========== Streamlit App ==========
def main():
    st.set_page_config(page_title="AI ë¹„ì„œ", layout="wide", page_icon="ğŸ¤–")

    with st.container():
        if Path("./chatbot_logo.png").exists():
            st.image("./chatbot_logo.png", use_container_width=True)
        st.markdown("---")
        st.title("ì•ˆë…•í•˜ì„¸ìš”! RAGë¥¼ í™œìš©í•œ 'AI ë¹„ì„œ í†¡í†¡ì´' ì…ë‹ˆë‹¤")

    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # ì‚¬ì´ë“œë°”: í‚¤ë§Œ ì…ë ¥(í´ë”/íŒŒì¼ ì…ë ¥ UIëŠ” ì—†ìŒ)
    with st.sidebar:
        st.session_state["OPENAI_API"] = st.text_input("OPENAI API í‚¤", placeholder="Enter Your API Key", type="password")
        st.session_state["SERPAPI_API"] = st.text_input("SERPAPI_API í‚¤ (ì„ íƒ)", placeholder="Enter Your API Key", type="password")
        if st.session_state.get("OPENAI_API"):
            os.environ["OPENAI_API_KEY"] = st.session_state["OPENAI_API"]
        if st.session_state.get("SERPAPI_API"):
            os.environ["SERPAPI_API_KEY"] = st.session_state["SERPAPI_API"]

        st.caption(f"ğŸ“ PDF ê²½ë¡œ: `{Path(PDF_DIR).resolve()}`")
        st.caption(f"ğŸ’¾ FAISS ì €ì¥ ê²½ë¡œ: `{Path(FAISS_DIR).resolve()}`")
        st.caption(f"ğŸ§  ì„ë² ë”© ëª¨ë¸: `{EMBED_MODEL}` / ì²­í¬ {CHUNK_SIZE} (+{CHUNK_OVERLAP})")

    # í‚¤ í™•ì¸
    if not st.session_state.get("OPENAI_API"):
        st.warning("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return

    # ë„êµ¬ êµ¬ì„±
    tools = []

    pdf_search_tool = make_pdf_search_tool()
    if pdf_search_tool:
        tools.append(pdf_search_tool)

    web_tool = search_web_tool_or_none()
    if web_tool:
        tools.append(web_tool)

    # LLM
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    # ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system",
             "Be sure to answer in Korean. You are a helpful assistant. "
             "Make sure to use the `pdf_search` tool for searching information from the pdf document. "
             "If you can't find the information from the PDF document, use the `web_search` tool for searching information from the web. "
             "If the userâ€™s question contains words like 'ìµœì‹ ', 'í˜„ì¬', or 'ì˜¤ëŠ˜', you must ALWAYS use the `web_search` tool to ensure real-time information is retrieved. "             "IMPORTANT: When you use the `pdf_search` tool to find information, you MUST always cite the source PDF file name in your response. "
             "Include the source information in a format like '(ì¶œì²˜: [íŒŒì¼ëª…])' at the end of the relevant sentence or paragraph. "             "Please always include emojis in your responses with a friendly tone. "
             "Your name is `AI ë¹„ì„œ í†¡í†¡ì´`. Please introduce yourself at the beginning of the conversation."
             ),
            ("placeholder", "{chat_history}"),
            ("human", "{input} \n\n Be sure to include emoji in your responses."),
            ("placeholder", "{agent_scratchpad}"),
        ]
    )

    agent = create_tool_calling_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    # ì…ë ¥ì°½
    user_input = st.chat_input("ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ê°€ìš”?")
    if user_input:
        # ë‹¨ìˆœ ë©”ì‹œì§€ ìŠ¤íƒ ìœ ì§€(ì´ˆê¸° ë¡œì§ ìœ ì§€)
        st.session_state["messages"].append({"role": "user", "content": user_input})
        prev_msgs = [{"role": m["role"], "content": m["content"]} for m in st.session_state["messages"][:-1]]
        response = chat_with_agent(user_input + "\n\nPrevious Messages: " + str(prev_msgs), agent_executor)
        st.session_state["messages"].append({"role": "assistant", "content": response})

    # ëŒ€í™” ì¶œë ¥
    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])


if __name__ == "__main__":
    main()
